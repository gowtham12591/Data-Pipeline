{\rtf1\ansi\ansicpg1252\cocoartf2708
\cocoatextscaling0\cocoaplatform0{\fonttbl\f0\fswiss\fcharset0 Helvetica;}
{\colortbl;\red255\green255\blue255;\red0\green0\blue0;\red34\green255\blue6;\red34\green255\blue6;
\red34\green255\blue6;}
{\*\expandedcolortbl;;\cssrgb\c0\c0\c0;\cssrgb\c0\c99452\c0;\cssrgb\c0\c100000\c0;
\cssrgb\c0\c99725\c0;}
\paperw11900\paperh16840\margl1440\margr1440\vieww11520\viewh8400\viewkind0
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f0\fs24 \cf0 \
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf2 Data Pipeline:\cf0 \
\
This project is all about performing Extract Load and then finally doing Transformation on the data. Basically ELT process has been followed.\
\
This example follows the below process,\
1. One part of dataset has been stored in Mysql data base and other part of the dataset has been created using Faker library in python. This is just to demonstrate the actual process of ELT. \
2. Both the dataset from Mysql db and Faker library has been ingested through the Messaging Queue Apache Kafka and pushed to \'91MongoDB\'92 which considered as a Data Warehouse for this example. \
3. Both the stored data from MongoDB has been ingested using python library pandas and then preprocessing of the data has been done.\
4. Finally the pre-processed data has been stored locally for further analysis.\
\
\cf2 Tools / Technology used:\cf0 \
Scripting Language: Python3.9.12\
Platform: On-Prem\
Backend: MySQL, MongoDB\
Messaging Queue: Apache Kafka\
Data Generator: Faker-Python\
\
}